{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np               #used for numerical operations\n",
    "import matplotlib.pyplot as plt  #used for plotting graphs   \n",
    "import pandas as pd             #used for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97a925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"/Applications/my_work/Digit_Recoqnizer/digit-recognizer/train.csv\")  #reading the csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "#convert to array for algebra\n",
    "df = np.array(df)\n",
    "#shape of the array, m gives the number of digits (rows) and n gives the number of pixels of each images (columns)\n",
    "m, n = df.shape\n",
    "print(m, n)\n",
    "\n",
    "#randomly shuffle the dataset, why? to make sure no bias in the data\n",
    "np.random.shuffle(df)\n",
    "\n",
    "#slices out the part of the dataset that will be used for validation and takes a transpose of the array\n",
    "data_dev = df[1000:m,:].T  # Fixed: use proper range for dev set\n",
    "\n",
    "# These lines extract the labels (Y_dev) and the features (X_dev) from the development set:\n",
    "# Y_dev is the first row after transpose (labels)\n",
    "# X_dev contains all other rows (features)\n",
    "Y_dev = data_dev[0]  # Fixed: get labels from transposed data\n",
    "X_dev = data_dev[1:n]  # Fixed: get features from transposed data\n",
    "X_dev = X_dev / 255.0  # normalize pixel values\n",
    "\n",
    "#training data\n",
    "data_train = df[0:1000,:].T  # Fixed: proper slicing\n",
    "Y_train = data_train[0]  # Fixed: get labels from transposed data\n",
    "X_train = data_train[1:n]  # Fixed: get features from transposed data\n",
    "X_train = X_train / 255.0  # normalize pixel values\n",
    "\n",
    "print(f\"Training set shape: X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "print(f\"Dev set shape: X_dev: {X_dev.shape}, Y_dev: {Y_dev.shape}\")\n",
    "\n",
    "# In summary, this code preprocesses a dataset for machine learning tasks involving image data. \n",
    "# It splits the dataset into training and validation sets, extracts features and labels, shuffles the data,\n",
    "# and normalizes the pixel values of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to train the NN \n",
    "\n",
    "#initialization function to get random weights and biases\n",
    "def init_params():\n",
    "    w1 = np.random.rand(10,784) - 0.5\n",
    "    b1 = np.random.rand(10,1) - 0.5\n",
    "    w2 = np.random.rand(10,10) - 0.5\n",
    "    b2 = np.random.rand(10,1) - 0.5\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "#softmax function to convert the output into a probability distribution\n",
    "def softmax(Z):\n",
    "    exp_z = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # Fixed: proper axis handling\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)  # Fixed: proper axis handling\n",
    "\n",
    "# This function converts class labels into a special format called \"one-hot encoding.\" \n",
    "# It creates an array where each row represents a different class, and each column represents a sample. \n",
    "# If a sample belongs to a class, the corresponding value in that class's row is set to 1; otherwise, it's set to 0.\n",
    "def one_hot(Y):\n",
    "    num_classes = np.max(Y) + 1\n",
    "    one_hot_Y = np.zeros((num_classes, Y.size))\n",
    "    one_hot_Y[Y, np.arange(Y.size)] = 1 #Its like saying put 1s in these specific spots of the matrix\n",
    "    return one_hot_Y\n",
    "\n",
    "# This function performs forward propagation in a neural network, computing the activations of hidden layers (A1) \n",
    "# and output layer (A2) given the input features (X) and learned parameters (weights W1 and W2, biases b1 and b2), \n",
    "# utilizing Rectified Linear Unit (ReLU) activation for the hidden layer and Softmax activation for the output layer.\n",
    "#we need z1 and z2 for gradient descent\n",
    "def forward_prop(w1, b1, w2, b2, X):\n",
    "    z1 = w1.dot(X) + b1\n",
    "    a1 = ReLU(z1)\n",
    "    z2 = w2.dot(a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# This function computes the gradients of the loss function with respect to the parameters (weights and biases) using backward propagation.\n",
    "# It calculates the gradients for both hidden and output layers (dW1, db1, dW2, db2) based on the given input (X), target labels (Y),\n",
    "# activations of the hidden layer (A1) and output layer (A2), and intermediate values (Z1, Z2) computed during forward propagation.\n",
    "# we are performing this on the entire batch of training data\n",
    "def back_prop(z1,a1,z2,a2,w2,X,Y):\n",
    "    m = Y.size #number of training examples\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dz2 = a2 - one_hot_Y\n",
    "    dw2 = 1/m * dz2.dot(a1.T) #T for array multiplication\n",
    "    db2 = 1/m * np.sum(dz2, axis=1, keepdims=True)  # Fixed: proper bias gradient calculation\n",
    "    dz1 = w2.T.dot(dz2) * ReLU_deriv(z1) # relu deriv returns 1 if z1 is positive, 0 otherwise\n",
    "    dw1 = 1/m * dz1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dz1, axis=1, keepdims=True)  # Fixed: proper bias gradient calculation\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "# This function updates the parameters (weights and biases) of the neural network using gradient descent to minimize the loss function, for learning \n",
    "def update_params(w1,b1,w2,b2,dw1,db1,dw2,db2,alpha):\n",
    "    w1 = w1 - alpha * dw1\n",
    "    b1 = b1 - alpha * db1\n",
    "    w2 = w2 - alpha * dw2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15ae5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2,0) #returns the index of the highest value in each column\n",
    "\n",
    "# This function calculates the accuracy of the predictions by comparing them with the true labels and dividing the number \n",
    "# of correct predictions by the total number of samples.\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "# This function implements gradient descent to train the neural network. It iteratively updates the parameters (weights and biases)\n",
    "# based on the gradients of the loss function with respect to these parameters. It prints the iteration number and accuracy every 10 iterations.\n",
    "# Finally, it returns the updated parameters.\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "    for i in range (iterations):\n",
    "        z1,a1,z2,a2 = forward_prop(w1,b1,w2,b2,X)\n",
    "        dw1, db1, dw2, db2 = back_prop(z1,a1,z2,a2,w2,X,Y)\n",
    "        w1, b1, w2, b2 = update_params(w1,b1,w2,b2,dw1,db1,dw2,db2,alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(a2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71af7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.101\n",
      "Iteration:  10\n",
      "0.118\n",
      "Iteration:  20\n",
      "0.087\n",
      "Iteration:  30\n",
      "0.093\n",
      "Iteration:  40\n",
      "0.092\n",
      "Iteration:  50\n",
      "0.092\n",
      "Iteration:  60\n",
      "0.092\n",
      "Iteration:  70\n",
      "0.092\n",
      "Iteration:  80\n",
      "0.092\n",
      "Iteration:  90\n",
      "0.092\n",
      "Iteration:  100\n",
      "0.092\n",
      "Iteration:  110\n",
      "0.092\n",
      "Iteration:  120\n",
      "0.092\n",
      "Iteration:  130\n",
      "0.092\n",
      "Iteration:  140\n",
      "0.092\n",
      "Iteration:  150\n",
      "0.092\n",
      "Iteration:  160\n",
      "0.092\n",
      "Iteration:  170\n",
      "0.092\n",
      "Iteration:  180\n",
      "0.092\n",
      "Iteration:  190\n",
      "0.092\n",
      "Iteration:  200\n",
      "0.092\n",
      "Iteration:  210\n",
      "0.092\n",
      "Iteration:  220\n",
      "0.092\n",
      "Iteration:  230\n",
      "0.092\n",
      "Iteration:  240\n",
      "0.092\n",
      "Iteration:  250\n",
      "0.092\n",
      "Iteration:  260\n",
      "0.092\n",
      "Iteration:  270\n",
      "0.092\n",
      "Iteration:  280\n",
      "0.092\n",
      "Iteration:  290\n",
      "0.092\n",
      "Iteration:  300\n",
      "0.092\n",
      "Iteration:  310\n",
      "0.092\n",
      "Iteration:  320\n",
      "0.092\n",
      "Iteration:  330\n",
      "0.092\n",
      "Iteration:  340\n",
      "0.092\n",
      "Iteration:  350\n",
      "0.092\n",
      "Iteration:  360\n",
      "0.092\n",
      "Iteration:  370\n",
      "0.092\n",
      "Iteration:  380\n",
      "0.092\n",
      "Iteration:  390\n",
      "0.092\n",
      "Iteration:  400\n",
      "0.092\n",
      "Iteration:  410\n",
      "0.092\n",
      "Iteration:  420\n",
      "0.092\n",
      "Iteration:  430\n",
      "0.092\n",
      "Iteration:  440\n",
      "0.092\n",
      "Iteration:  450\n",
      "0.092\n",
      "Iteration:  460\n",
      "0.092\n",
      "Iteration:  470\n",
      "0.092\n",
      "Iteration:  480\n",
      "0.092\n",
      "Iteration:  490\n",
      "0.092\n",
      "Iteration:  500\n",
      "0.092\n",
      "Iteration:  510\n",
      "0.092\n",
      "Iteration:  520\n",
      "0.092\n",
      "Iteration:  530\n",
      "0.092\n",
      "Iteration:  540\n",
      "0.092\n",
      "Iteration:  550\n",
      "0.092\n",
      "Iteration:  560\n",
      "0.092\n",
      "Iteration:  570\n",
      "0.092\n",
      "Iteration:  580\n",
      "0.092\n",
      "Iteration:  590\n",
      "0.092\n",
      "Iteration:  600\n",
      "0.092\n",
      "Iteration:  610\n",
      "0.092\n",
      "Iteration:  620\n",
      "0.092\n",
      "Iteration:  630\n",
      "0.092\n",
      "Iteration:  640\n",
      "0.092\n",
      "Iteration:  650\n",
      "0.092\n",
      "Iteration:  660\n",
      "0.092\n",
      "Iteration:  670\n",
      "0.092\n",
      "Iteration:  680\n",
      "0.092\n",
      "Iteration:  690\n",
      "0.092\n",
      "Iteration:  700\n",
      "0.092\n",
      "Iteration:  710\n",
      "0.092\n",
      "Iteration:  720\n",
      "0.092\n",
      "Iteration:  730\n",
      "0.092\n",
      "Iteration:  740\n",
      "0.092\n",
      "Iteration:  750\n",
      "0.092\n",
      "Iteration:  760\n",
      "0.092\n",
      "Iteration:  770\n",
      "0.092\n",
      "Iteration:  780\n",
      "0.092\n",
      "Iteration:  790\n",
      "0.092\n",
      "Iteration:  800\n",
      "0.092\n",
      "Iteration:  810\n",
      "0.092\n",
      "Iteration:  820\n",
      "0.092\n",
      "Iteration:  830\n",
      "0.092\n",
      "Iteration:  840\n",
      "0.092\n",
      "Iteration:  850\n",
      "0.092\n",
      "Iteration:  860\n",
      "0.092\n",
      "Iteration:  870\n",
      "0.092\n",
      "Iteration:  880\n",
      "0.092\n",
      "Iteration:  890\n",
      "0.092\n",
      "Iteration:  900\n",
      "0.092\n",
      "Iteration:  910\n",
      "0.092\n",
      "Iteration:  920\n",
      "0.092\n",
      "Iteration:  930\n",
      "0.092\n",
      "Iteration:  940\n",
      "0.092\n",
      "Iteration:  950\n",
      "0.092\n",
      "Iteration:  960\n",
      "0.092\n",
      "Iteration:  970\n",
      "0.092\n",
      "Iteration:  980\n",
      "0.092\n",
      "Iteration:  990\n",
      "0.092\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b65188db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates predictions using the input data and the trained parameters.\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "# This function tests a single prediction using the trained parameters on a specific index of the dataset. \n",
    "# It prints the predicted label and the actual label.\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label) \n",
    "    # This code reshapes the image data to its original 28x28 dimensions and then visualizes it \n",
    "    # using matplotlib's imshow function, displaying the grayscale image.\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5faaa4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0]\n",
      "Label:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function tests the prediction for a specific index in the training dataset using the trained parameters W1, b1, W2, and b2. \n",
    "# It prints the prediction, actual label, and displays the corresponding image.\n",
    "\n",
    "test_prediction(2, W1, b1, W2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c0beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
