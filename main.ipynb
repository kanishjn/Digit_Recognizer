{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94d2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np               #used for numerical operations\n",
    "import matplotlib.pyplot as plt  #used for plotting graphs   \n",
    "import pandas as pd             #used for data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"add file path\")  #reading the csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58d9cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n",
      "Training set shape: X_train: (784, 1000), Y_train: (1000,)\n",
      "Dev set shape: X_dev: (784, 41000), Y_dev: (41000,)\n"
     ]
    }
   ],
   "source": [
    "#convert to array for algebra\n",
    "df = np.array(df)\n",
    "#shape of the array, m gives the number of digits (rows) and n gives the number of pixels of each images (columns)\n",
    "m, n = df.shape\n",
    "print(m, n)\n",
    "\n",
    "#randomly shuffle the dataset, why? to make sure no bias in the data\n",
    "np.random.shuffle(df)\n",
    "\n",
    "#slices out the part of the dataset that will be used for validation and takes a transpose of the array\n",
    "data_dev = df[1000:m,:].T  # Fixed: use proper range for dev set\n",
    "\n",
    "# These lines extract the labels (Y_dev) and the features (X_dev) from the development set:\n",
    "# Y_dev is the first row after transpose (labels)\n",
    "# X_dev contains all other rows (features)\n",
    "Y_dev = data_dev[0]  # Fixed: get labels from transposed data\n",
    "X_dev = data_dev[1:n]  # Fixed: get features from transposed data\n",
    "X_dev = X_dev / 255.0  # normalize pixel values\n",
    "\n",
    "#training data\n",
    "data_train = df[0:1000,:].T  # Fixed: proper slicing\n",
    "Y_train = data_train[0]  # Fixed: get labels from transposed data\n",
    "X_train = data_train[1:n]  # Fixed: get features from transposed data\n",
    "X_train = X_train / 255.0  # normalize pixel values\n",
    "\n",
    "print(f\"Training set shape: X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "print(f\"Dev set shape: X_dev: {X_dev.shape}, Y_dev: {Y_dev.shape}\")\n",
    "\n",
    "# In summary, this code preprocesses a dataset for machine learning tasks involving image data. \n",
    "# It splits the dataset into training and validation sets, extracts features and labels, shuffles the data,\n",
    "# and normalizes the pixel values of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5e7c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to train the NN \n",
    "\n",
    "#initialization function to get random weights and biases\n",
    "def init_params():\n",
    "    w1 = np.random.rand(16,784) - 0.5\n",
    "    b1 = np.random.rand(16,1) - 0.5\n",
    "    w2 = np.random.rand(10,16) - 0.5\n",
    "    b2 = np.random.rand(10,1) - 0.5\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "#softmax function to convert the output into a probability distribution\n",
    "def softmax(Z):\n",
    "    exp_z = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # Fixed: proper axis handling\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)  # Fixed: proper axis handling\n",
    "\n",
    "# This function converts class labels into a special format called \"one-hot encoding.\" \n",
    "# It creates an array where each row represents a different class, and each column represents a sample. \n",
    "# If a sample belongs to a class, the corresponding value in that class's row is set to 1; otherwise, it's set to 0.\n",
    "def one_hot(Y):\n",
    "    num_classes = np.max(Y) + 1\n",
    "    one_hot_Y = np.zeros((num_classes, Y.size))\n",
    "    one_hot_Y[Y, np.arange(Y.size)] = 1 #Its like saying put 1s in these specific spots of the matrix\n",
    "    return one_hot_Y\n",
    "\n",
    "# This function performs forward propagation in a neural network, computing the activations of hidden layers (A1) \n",
    "# and output layer (A2) given the input features (X) and learned parameters (weights W1 and W2, biases b1 and b2), \n",
    "# utilizing Rectified Linear Unit (ReLU) activation for the hidden layer and Softmax activation for the output layer.\n",
    "#we need z1 and z2 for gradient descent\n",
    "def forward_prop(w1, b1, w2, b2, X):\n",
    "    z1 = w1.dot(X) + b1\n",
    "    a1 = ReLU(z1)\n",
    "    z2 = w2.dot(a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# This function computes the gradients of the loss function with respect to the parameters (weights and biases) using backward propagation.\n",
    "# It calculates the gradients for both hidden and output layers (dW1, db1, dW2, db2) based on the given input (X), target labels (Y),\n",
    "# activations of the hidden layer (A1) and output layer (A2), and intermediate values (Z1, Z2) computed during forward propagation.\n",
    "# we are performing this on the entire batch of training data\n",
    "def back_prop(z1,a1,z2,a2,w2,X,Y):\n",
    "    m = Y.size #number of training examples\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dz2 = a2 - one_hot_Y\n",
    "    dw2 = 1/m * dz2.dot(a1.T) #T for array multiplication\n",
    "    db2 = 1/m * np.sum(dz2, axis=1, keepdims=True)  # Fixed: proper bias gradient calculation\n",
    "    dz1 = w2.T.dot(dz2) * ReLU_deriv(z1) # relu deriv returns 1 if z1 is positive, 0 otherwise\n",
    "    dw1 = 1/m * dz1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dz1, axis=1, keepdims=True)  # Fixed: proper bias gradient calculation\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "# This function updates the parameters (weights and biases) of the neural network using gradient descent to minimize the loss function, for learning \n",
    "def update_params(w1,b1,w2,b2,dw1,db1,dw2,db2,alpha):\n",
    "    w1 = w1 - alpha * dw1\n",
    "    b1 = b1 - alpha * db1\n",
    "    w2 = w2 - alpha * dw2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b15ae5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2,0) #returns the index of the highest value in each column\n",
    "\n",
    "# This function calculates the accuracy of the predictions by comparing them with the true labels and dividing the number \n",
    "# of correct predictions by the total number of samples.\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "# This function implements gradient descent to train the neural network. It iteratively updates the parameters (weights and biases)\n",
    "# based on the gradients of the loss function with respect to these parameters. It prints the iteration number and accuracy every 10 iterations.\n",
    "# Finally, it returns the updated parameters.\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "    for i in range (iterations):\n",
    "        z1,a1,z2,a2 = forward_prop(w1,b1,w2,b2,X)\n",
    "        dw1, db1, dw2, db2 = back_prop(z1,a1,z2,a2,w2,X,Y)\n",
    "        w1, b1, w2, b2 = update_params(w1,b1,w2,b2,dw1,db1,dw2,db2,alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(a2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e71af7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.098\n",
      "Iteration:  10\n",
      "0.206\n",
      "Iteration:  20\n",
      "0.317\n",
      "Iteration:  30\n",
      "0.419\n",
      "Iteration:  40\n",
      "0.481\n",
      "Iteration:  50\n",
      "0.543\n",
      "Iteration:  60\n",
      "0.579\n",
      "Iteration:  70\n",
      "0.617\n",
      "Iteration:  80\n",
      "0.644\n",
      "Iteration:  90\n",
      "0.668\n",
      "Iteration:  100\n",
      "0.703\n",
      "Iteration:  110\n",
      "0.723\n",
      "Iteration:  120\n",
      "0.736\n",
      "Iteration:  130\n",
      "0.746\n",
      "Iteration:  140\n",
      "0.759\n",
      "Iteration:  150\n",
      "0.77\n",
      "Iteration:  160\n",
      "0.782\n",
      "Iteration:  170\n",
      "0.79\n",
      "Iteration:  180\n",
      "0.796\n",
      "Iteration:  190\n",
      "0.805\n",
      "Iteration:  200\n",
      "0.811\n",
      "Iteration:  210\n",
      "0.815\n",
      "Iteration:  220\n",
      "0.827\n",
      "Iteration:  230\n",
      "0.83\n",
      "Iteration:  240\n",
      "0.833\n",
      "Iteration:  250\n",
      "0.84\n",
      "Iteration:  260\n",
      "0.849\n",
      "Iteration:  270\n",
      "0.855\n",
      "Iteration:  280\n",
      "0.859\n",
      "Iteration:  290\n",
      "0.865\n",
      "Iteration:  300\n",
      "0.867\n",
      "Iteration:  310\n",
      "0.875\n",
      "Iteration:  320\n",
      "0.876\n",
      "Iteration:  330\n",
      "0.879\n",
      "Iteration:  340\n",
      "0.882\n",
      "Iteration:  350\n",
      "0.882\n",
      "Iteration:  360\n",
      "0.886\n",
      "Iteration:  370\n",
      "0.89\n",
      "Iteration:  380\n",
      "0.893\n",
      "Iteration:  390\n",
      "0.894\n",
      "Iteration:  400\n",
      "0.896\n",
      "Iteration:  410\n",
      "0.898\n",
      "Iteration:  420\n",
      "0.902\n",
      "Iteration:  430\n",
      "0.907\n",
      "Iteration:  440\n",
      "0.909\n",
      "Iteration:  450\n",
      "0.911\n",
      "Iteration:  460\n",
      "0.913\n",
      "Iteration:  470\n",
      "0.916\n",
      "Iteration:  480\n",
      "0.917\n",
      "Iteration:  490\n",
      "0.92\n",
      "Iteration:  500\n",
      "0.921\n",
      "Iteration:  510\n",
      "0.924\n",
      "Iteration:  520\n",
      "0.925\n",
      "Iteration:  530\n",
      "0.928\n",
      "Iteration:  540\n",
      "0.93\n",
      "Iteration:  550\n",
      "0.933\n",
      "Iteration:  560\n",
      "0.936\n",
      "Iteration:  570\n",
      "0.937\n",
      "Iteration:  580\n",
      "0.941\n",
      "Iteration:  590\n",
      "0.942\n",
      "Iteration:  600\n",
      "0.943\n",
      "Iteration:  610\n",
      "0.945\n",
      "Iteration:  620\n",
      "0.947\n",
      "Iteration:  630\n",
      "0.947\n",
      "Iteration:  640\n",
      "0.948\n",
      "Iteration:  650\n",
      "0.949\n",
      "Iteration:  660\n",
      "0.949\n",
      "Iteration:  670\n",
      "0.951\n",
      "Iteration:  680\n",
      "0.954\n",
      "Iteration:  690\n",
      "0.956\n",
      "Iteration:  700\n",
      "0.956\n",
      "Iteration:  710\n",
      "0.956\n",
      "Iteration:  720\n",
      "0.958\n",
      "Iteration:  730\n",
      "0.958\n",
      "Iteration:  740\n",
      "0.958\n",
      "Iteration:  750\n",
      "0.958\n",
      "Iteration:  760\n",
      "0.959\n",
      "Iteration:  770\n",
      "0.959\n",
      "Iteration:  780\n",
      "0.959\n",
      "Iteration:  790\n",
      "0.959\n",
      "Iteration:  800\n",
      "0.96\n",
      "Iteration:  810\n",
      "0.961\n",
      "Iteration:  820\n",
      "0.962\n",
      "Iteration:  830\n",
      "0.962\n",
      "Iteration:  840\n",
      "0.963\n",
      "Iteration:  850\n",
      "0.966\n",
      "Iteration:  860\n",
      "0.967\n",
      "Iteration:  870\n",
      "0.968\n",
      "Iteration:  880\n",
      "0.968\n",
      "Iteration:  890\n",
      "0.97\n",
      "Iteration:  900\n",
      "0.972\n",
      "Iteration:  910\n",
      "0.973\n",
      "Iteration:  920\n",
      "0.974\n",
      "Iteration:  930\n",
      "0.975\n",
      "Iteration:  940\n",
      "0.976\n",
      "Iteration:  950\n",
      "0.977\n",
      "Iteration:  960\n",
      "0.979\n",
      "Iteration:  970\n",
      "0.979\n",
      "Iteration:  980\n",
      "0.98\n",
      "Iteration:  990\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b65188db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates predictions using the input data and the trained parameters.\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "# This function tests a single prediction using the trained parameters on a specific index of the dataset. \n",
    "# It prints the predicted label and the actual label.\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label) \n",
    "    # This code reshapes the image data to its original 28x28 dimensions and then visualizes it \n",
    "    # using matplotlib's imshow function, displaying the grayscale image.\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5faaa4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [5]\n",
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function tests the prediction for a specific index in the training dataset using the trained parameters W1, b1, W2, and b2. \n",
    "# It prints the prediction, actual label, and displays the corresponding image.\n",
    "test_prediction(95, W1, b1, W2, b2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
